{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2de08e5-0689-4e26-961f-79d8dc71ccf0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Load and store the data\n",
    "\n",
    "We loaded our data with DataBricks inside a table called: web_Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0f92abf-695c-4102-8901-4b3b654aa09f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pyspark\n",
    "from pyspark.sql.functions import expr, col, min as spark_min, collect_list, when, size, explode, lit, array_sort, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f233d288-9bf3-4fe6-82a2-ddd773bae73c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1181126391222697>:2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#get the raw data and clean it \u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mtable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mweb_Google\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_c0\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;66;03m#\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwhere(\u001B[38;5;241m~\u001B[39mdf[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_c0\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mcontains(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m#\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#df.show(10)\u001B[39;00m\n",
       "\u001B[1;32m      5\u001B[0m \n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Split the '_c0' column into two columns based on the tab character '\\t'\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:473\u001B[0m, in \u001B[0;36mDataFrameReader.table\u001B[0;34m(self, tableName)\u001B[0m\n",
       "\u001B[1;32m    439\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtable\u001B[39m(\u001B[38;5;28mself\u001B[39m, tableName: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m    440\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns the specified table as a :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m    441\u001B[0m \n",
       "\u001B[1;32m    442\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.4.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    471\u001B[0m \u001B[38;5;124;03m    >>> _ = spark.sql(\"DROP TABLE tblA\")\u001B[39;00m\n",
       "\u001B[1;32m    472\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtableName\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `web_Google` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
       "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
       "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n",
       "'UnresolvedRelation [web_Google], [], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-1181126391222697>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#get the raw data and clean it \u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m df \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mtable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mweb_Google\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_c0\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwhere(\u001B[38;5;241m~\u001B[39mdf[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_c0\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mcontains(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m#\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#df.show(10)\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Split the '_c0' column into two columns based on the tab character '\\t'\u001B[39;00m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:473\u001B[0m, in \u001B[0;36mDataFrameReader.table\u001B[0;34m(self, tableName)\u001B[0m\n\u001B[1;32m    439\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtable\u001B[39m(\u001B[38;5;28mself\u001B[39m, tableName: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    440\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns the specified table as a :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m    441\u001B[0m \n\u001B[1;32m    442\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.4.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    471\u001B[0m \u001B[38;5;124;03m    >>> _ = spark.sql(\"DROP TABLE tblA\")\u001B[39;00m\n\u001B[1;32m    472\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtableName\u001B[49m\u001B[43m)\u001B[49m)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `web_Google` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [web_Google], [], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `web_Google` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [web_Google], [], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get the raw data and clean it \n",
    "df = spark.read.table(\"web_Google\").select('_c0')#\n",
    "df = df.where(~df['_c0'].contains('#'))\n",
    "#df.show(10)\n",
    "\n",
    "# Split the '_c0' column into two columns based on the tab character '\\t'\n",
    "df = df.withColumn(\"key\", split(df[\"_c0\"], \"\\t\").getItem(0))\n",
    "df = df.withColumn(\"values\", split(df[\"_c0\"], \"\\t\").getItem(1)).drop('_c0')\n",
    "\n",
    "df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80e2386d-16f0-4952-9c9b-f69d8dc8cc83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Select the \"key\" and \"value\" columns from the DataFrame\n",
    "rdd = df.select(\"key\", \"values\").rdd.map(lambda x: [int(x[0]), int(x[1])])\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84f3dc25-0d47-4455-b647-6ad03121bcbf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Connected Component Computation \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4abe9586-53cd-4832-911d-5a24f0a3a332",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##RDD implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "461ac847-8826-4a02-8f31-88b2a9dccae0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#cf iterate \n",
    "def ccf_iterate_rdd(rdd):\n",
    "    rdd = rdd.union(rdd.map(lambda x: (x[1], x[0])))\n",
    "    rdd = rdd.groupByKey()\n",
    "\n",
    "    def new_pairs(key, values):\n",
    "        min_val = key\n",
    "        value_list = []\n",
    "        new_rdd = []\n",
    "        c = 0\n",
    "        for value in values:\n",
    "            if value < min_val:\n",
    "                min_val = value\n",
    "            value_list.append(value)\n",
    "\n",
    "        if min_val < key:\n",
    "            new_rdd = [(key, min_val)]\n",
    "            for value in value_list:\n",
    "                if not min_val == value:\n",
    "                    c =+1\n",
    "                    new_rdd.append((value, min_val))\n",
    "        \n",
    "        return (key, new_rdd, c)\n",
    "\n",
    "    rdd = rdd.map(lambda x: new_pairs(x[0], x[1]))\n",
    "    c = rdd.map(lambda x: x[2]).sum()\n",
    "\n",
    "    rdd = rdd.flatMap(lambda x: x[1])\n",
    "\n",
    "    return rdd, c\n",
    "\n",
    "def ccf_dedup_rdd(rdd):\n",
    "    #remove duplicate \n",
    "    rdd = rdd.map(lambda x: (x, None))\n",
    "    rdd = rdd.groupByKey()\n",
    "    rdd = rdd.map(lambda x: x[0])\n",
    "    return rdd\n",
    "\n",
    "#ccf iterate sorting\n",
    "def ccf_iterate_sorting_rdd(rdd):\n",
    "    rdd = rdd.union(rdd.map(lambda x: (x[1], x[0])))\n",
    "    rdd = rdd.groupByKey().mapValues(lambda x: sorted(x))\n",
    "\n",
    "    def new_pairs(key, values):\n",
    "        min_val = values[0]\n",
    "        new_rdd = []\n",
    "        c = 0\n",
    "        if min_val < key:\n",
    "            new_rdd.append((key, min_val))\n",
    "            for value in values:\n",
    "                if not value == min_val:\n",
    "                    c =+1\n",
    "                    new_rdd.append((value, min_val))\n",
    "\n",
    "        return (key, new_rdd, c)\n",
    "\n",
    "    rdd = rdd.map(lambda x: new_pairs(x[0], x[1]))\n",
    "    c = rdd.map(lambda x: x[2]).sum()\n",
    "\n",
    "    rdd = rdd.flatMap(lambda x: x[1])\n",
    "\n",
    "    return rdd, c\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31f92dff-f9aa-43f1-be94-8f5d168afa1d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##DataFrame implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c07a25a3-655b-4f3a-939b-dfa52c5f530f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def ccf_iterate_df(df):\n",
    "\n",
    "    df = df.union(df.select([\"values\", \"key\"]))\n",
    "    df = df.groupBy(\"key\").agg(spark_min(\"values\").alias(\"min_val\"), collect_list(\"values\").alias(\"values\"))\n",
    "    df = df.filter(col(\"min_val\") < col(\"key\"))\n",
    "\n",
    "    #emit 1\n",
    "    df_min = df.select(col(\"key\"), col(\"min_val\").alias(\"values\"))\n",
    "\n",
    "    df = df.select(\"key\", \"min_val\", explode(\"values\").alias(\"values\")).filter(col(\"min_val\") != col(\"values\"))\n",
    "\n",
    "    #emit 2\n",
    "    df_new_pairs = df.select(col(\"values\").alias(\"key\"), col(\"min_val\").alias(\"values\"))\n",
    "\n",
    "    df_result = df_min.union(df_new_pairs)\n",
    "    c = df_new_pairs.count()\n",
    "\n",
    "    return df_result, c\n",
    "\n",
    "\n",
    "def ccf_dedup_df(df):\n",
    "    df = df.dropDuplicates()\n",
    "    return df\n",
    "\n",
    "def ccf_iterate_sorting_df(df):\n",
    "\n",
    "    df = df.union(df.select([\"values\", \"key\"]))\n",
    "    df = df.groupBy(\"key\").agg(array_sort(collect_list(\"values\")).alias(\"value_list\"))\n",
    "    df = df.withColumn(\"min_val\", col(\"value_list\")[0])\n",
    "    \n",
    "    df = df.filter(col(\"min_val\") < col(\"key\"))\n",
    "\n",
    "    #emit1\n",
    "    df_min = df.select(col(\"key\"), col(\"min_val\").alias(\"values\"))\n",
    "    \n",
    "    #emit 2\n",
    "    df_new_pairs = df.withColumn(\"values\", explode(col(\"value_list\")))\n",
    "    df_new_pairs = df_new_pairs.filter(col(\"min_val\") < col(\"values\")).select(col(\"values\").alias(\"key\"), col(\"min_val\").alias(\"values\"))\n",
    "    \n",
    "    c = df_new_pairs.count()\n",
    "    # Union the two DataFrames\n",
    "    df_result = df_min.union(df_new_pairs)\n",
    "\n",
    "    return df_result, c\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd455266-d4c9-4e1b-ae33-2d5f4bdc3c3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cc84a4e-6a0c-46d5-a8f9-94503217a1f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main_loop_rdd(rdd, max_iteration, sorting=False):\n",
    "    c = 1\n",
    "    k = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    while c > 0:\n",
    "        if not sorting:\n",
    "            rdd, c = ccf_iterate_rdd(rdd)\n",
    "        else: \n",
    "            rdd, c = ccf_iterate_sorting_rdd(rdd)\n",
    "        rdd = ccf_dedup_rdd(rdd)\n",
    "        #print(\"iter\", k)\n",
    "        k = k + 1\n",
    "        #print('Iteration', k)\n",
    "        if k > max_iteration:\n",
    "\n",
    "            print(\"EXCEEDED MAX NUMBER OF ITERATION\")\n",
    "            break \n",
    "    #print(\"ended while\")\n",
    "    time_ccf = time.time() - start_time\n",
    "\n",
    "    return rdd, time_ccf, k\n",
    "\n",
    "def main_loop_df(df, max_iteration, sorting=False):\n",
    "    c = 1\n",
    "    k = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    while c > 0:\n",
    "        if not sorting:\n",
    "            df, c = ccf_iterate_df(df)\n",
    "        else: \n",
    "            df, c = ccf_iterate_sorting_df(df)\n",
    "        df = ccf_dedup_df(df)\n",
    "        #print(\"iter\", k)\n",
    "        k = k + 1\n",
    "        #print('Iteration df', k)\n",
    "        if k > max_iteration:\n",
    "            print(\"EXCEEDED MAX NUMBER OF ITERATION\")\n",
    "            break \n",
    "    #print(\"Ended while\")\n",
    "    time_ccf = time.time() - start_time\n",
    "\n",
    "    return df, time_ccf, k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14607e71-e2f8-4ebb-bbd7-5c86decded02",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Sanity Check\n",
    "\n",
    "We run a test on the graph presented in the white paper. It should take 4 iterations to parse it. \n",
    "And the out put should be: (2,1), (3,1), (4,1), (5,1), (7,6), (8,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "642b2ad7-fde8-414b-9669-ae9515ccf3ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#sanity check\n",
    "rdd_test = sc.parallelize([(1, 2), (2, 3), (2, 4), (4, 5), (6, 7), (7, 8)])\n",
    "df_test = rdd_test.toDF([\"key\", \"values\"])\n",
    "\n",
    "df_out, time_, k = main_loop_df(df_test, 10, True)\n",
    "\n",
    "print(\"Iterations: \", k)\n",
    "#df_out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fa246a6-1cde-4216-be08-dbc25441aa61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0756fa74-f8c7-4898-8a54-57a6be57d4f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd_out, time_, k = main_loop_rdd(rdd_test, 10, False)\n",
    "\n",
    "print(\"Iterations: \", k)\n",
    "rdd_out.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb9bb7a-5af6-4deb-8bf3-2c2d5f49f222",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Experimental Analysis\n",
    "Experimental analysis comparing the RDD and DataFrame versions\n",
    "is conducted on graphs of increasing size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc5606fc-0b44-4ad0-8962-74ffd9715e9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Run modulations of our program on graphs of increasing size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c1f032f-ec86-4c6e-ba94-5d5bab4551f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = {'DataType': [], 'Sorting': [], 'Size': [],'Iterations': [], 'Time(s)': []}\n",
    "\n",
    "max_iterate = 10000\n",
    "data_size = 1000\n",
    "\n",
    "\n",
    "#increase the size of the rrd to measure impact in time \n",
    "for n in np.arange(1, data_size, step=100):\n",
    "\n",
    "    df_sample = df.limit(int(n))\n",
    "    rdd_sample = df_sample.rdd\n",
    "\n",
    "    #rdd without sorting\n",
    "    #print(\"start iter rdd no sort\")\n",
    "    rdd_out, time_ccf, k = main_loop_rdd(rdd_sample, max_iterate, False)\n",
    "\n",
    "    results['DataType'].append('RDD')\n",
    "    results['Sorting'].append('False')\n",
    "    results['Iterations'].append(k)\n",
    "    results['Time(s)'].append(time_ccf)\n",
    "    results[\"Size\"].append(n)\n",
    "\n",
    "    #rdd with sorting\n",
    "    #print(\"start iter rdd sort\")\n",
    "    rdd_out, time_ccf, k = main_loop_rdd(rdd_sample, max_iterate, True)\n",
    "\n",
    "    results['DataType'].append('RDD')\n",
    "    results['Sorting'].append('True')\n",
    "    results['Iterations'].append(k)\n",
    "    results['Time(s)'].append(time_ccf)\n",
    "    results[\"Size\"].append(n)\n",
    "\n",
    "    #DF without sorting\n",
    "    #print(\"start iter df\")\n",
    "    df_out, time_ccf, k = main_loop_df(df_sample, max_iterate, False)\n",
    "\n",
    "    results['DataType'].append('DataFrame')\n",
    "    results['Sorting'].append('False')\n",
    "    results['Iterations'].append(k)\n",
    "    results['Time(s)'].append(time_ccf)\n",
    "    results[\"Size\"].append(n)\n",
    "\n",
    "    #DF with sorting\n",
    "    #print(\"start iter df sort\")\n",
    "    df_out, time_ccf, k = main_loop_df(df_sample, max_iterate, True)\n",
    "\n",
    "    results['DataType'].append('DataFrame')\n",
    "    results['Sorting'].append('True')\n",
    "    results['Iterations'].append(k)\n",
    "    results['Time(s)'].append(time_ccf)\n",
    "    results[\"Size\"].append(n)\n",
    "\n",
    "    print(\"Sample size\", n, \"parsed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55ad6544-cbb4-4c0f-8887-fa6d95ce6098",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Display results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4178f86b-f2d3-4db5-8dfb-e04ae14a93b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bc613c5-08c5-4dfc-97c8-b02577a30783",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visual analysis \n",
    "\n",
    "# Define a dictionary to map unique values of 'DataType' to colors\n",
    "data_type_colors = {'RDD': 'blue', 'DataFrame': 'green'}\n",
    "\n",
    "# Define a dictionary to map unique values of 'Sort' to marker shapes\n",
    "sort_markers = {'True': 'o', 'False': 'x'}\n",
    "\n",
    "# Create a figure and axis for the plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Iterate through unique combinations of 'DataType' and 'Sort'\n",
    "for data_type, group1 in results_df.groupby('DataType'):\n",
    "    for sort, group2 in group1.groupby('Sorting'):\n",
    "        marker = sort_markers.get(sort, 'o')\n",
    "        color = data_type_colors.get(data_type, 'gray')\n",
    "        label = f'{data_type}, {sort}'\n",
    "        ax.scatter(group2['Size'], group2['Time(s)'], c=color, marker=marker, label=label)\n",
    "\n",
    "# Set labels and legend\n",
    "ax.set_xlabel('Size')\n",
    "ax.set_ylabel('Time')\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24095247-c152-4ca0-b94a-68014b73de96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "David Benayoun_Bigdata_CCF",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
