{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2de08e5-0689-4e26-961f-79d8dc71ccf0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Load and store the data\n",
    "\n",
    "We loaded our data with DataBricks inside a table called: web_Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0f92abf-695c-4102-8901-4b3b654aa09f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pyspark\n",
    "from pyspark.sql.functions import expr, col, min as spark_min, collect_list, when, size, explode, lit, array_sort, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f233d288-9bf3-4fe6-82a2-ddd773bae73c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#get the raw data and clean it \n",
    "df = spark.read.table(\"web_Google\").select('_c0')#\n",
    "df = df.where(~df['_c0'].contains('#'))\n",
    "#df.show(10)\n",
    "\n",
    "# Split the '_c0' column into two columns based on the tab character '\\t'\n",
    "df = df.withColumn(\"key\", split(df[\"_c0\"], \"\\t\").getItem(0))\n",
    "df = df.withColumn(\"values\", split(df[\"_c0\"], \"\\t\").getItem(1)).drop('_c0')\n",
    "\n",
    "#df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80e2386d-16f0-4952-9c9b-f69d8dc8cc83",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select the \"key\" and \"value\" columns from the DataFrame\n",
    "rdd = df.select(\"key\", \"values\").rdd.map(lambda x: [int(x[0]), int(x[1])])\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84f3dc25-0d47-4455-b647-6ad03121bcbf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Connected Component Computation \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4abe9586-53cd-4832-911d-5a24f0a3a332",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##RDD implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "461ac847-8826-4a02-8f31-88b2a9dccae0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#cf iterate \n",
    "def ccf_iterate_rdd(rdd):\n",
    "    rdd = rdd.union(rdd.map(lambda x: (x[1], x[0])))\n",
    "    rdd = rdd.groupByKey()\n",
    "\n",
    "    def new_pairs(key, values):\n",
    "        min_val = key\n",
    "        value_list = []\n",
    "        new_rdd = []\n",
    "        c = 0\n",
    "        for value in values:\n",
    "            if value < min_val:\n",
    "                min_val = value\n",
    "            value_list.append(value)\n",
    "\n",
    "        if min_val < key:\n",
    "            new_rdd = [(key, min_val)]\n",
    "            for value in value_list:\n",
    "                if not min_val == value:\n",
    "                    c =+1\n",
    "                    new_rdd.append((value, min_val))\n",
    "        \n",
    "        return (key, new_rdd, c)\n",
    "\n",
    "    rdd = rdd.map(lambda x: new_pairs(x[0], x[1]))\n",
    "    c = rdd.map(lambda x: x[2]).sum()\n",
    "\n",
    "    rdd = rdd.flatMap(lambda x: x[1])\n",
    "\n",
    "    return rdd, c\n",
    "\n",
    "def ccf_dedup_rdd(rdd):\n",
    "    #remove duplicate \n",
    "    rdd = rdd.map(lambda x: (x, None))\n",
    "    rdd = rdd.groupByKey()\n",
    "    rdd = rdd.map(lambda x: x[0])\n",
    "    return rdd\n",
    "\n",
    "#ccf iterate sorting\n",
    "def ccf_iterate_sorting_rdd(rdd):\n",
    "    rdd = rdd.union(rdd.map(lambda x: (x[1], x[0])))\n",
    "    rdd = rdd.groupByKey().mapValues(lambda x: sorted(x))\n",
    "\n",
    "    def new_pairs(key, values):\n",
    "        min_val = values[0]\n",
    "        new_rdd = []\n",
    "        c = 0\n",
    "        if min_val < key:\n",
    "            new_rdd.append((key, min_val))\n",
    "            for value in values:\n",
    "                if not value == min_val:\n",
    "                    c =+1\n",
    "                    new_rdd.append((value, min_val))\n",
    "\n",
    "        return (key, new_rdd, c)\n",
    "\n",
    "    rdd = rdd.map(lambda x: new_pairs(x[0], x[1]))\n",
    "    c = rdd.map(lambda x: x[2]).sum()\n",
    "\n",
    "    rdd = rdd.flatMap(lambda x: x[1])\n",
    "\n",
    "    return rdd, c\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31f92dff-f9aa-43f1-be94-8f5d168afa1d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##DataFrame implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c07a25a3-655b-4f3a-939b-dfa52c5f530f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ccf_iterate_df(df):\n",
    "\n",
    "    df = df.union(df.select([\"values\", \"key\"]))\n",
    "    df = df.groupBy(\"key\").agg(spark_min(\"values\").alias(\"min_val\"), collect_list(\"values\").alias(\"values\"))\n",
    "    df = df.filter(col(\"min_val\") < col(\"key\"))\n",
    "\n",
    "    #emit 1\n",
    "    df_min = df.select(col(\"key\"), col(\"min_val\").alias(\"values\"))\n",
    "\n",
    "    df = df.select(\"key\", \"min_val\", explode(\"values\").alias(\"values\")).filter(col(\"min_val\") != col(\"values\"))\n",
    "\n",
    "    #emit 2\n",
    "    df_new_pairs = df.select(col(\"values\").alias(\"key\"), col(\"min_val\").alias(\"values\"))\n",
    "\n",
    "    df_result = df_min.union(df_new_pairs)\n",
    "    c = df_new_pairs.count()\n",
    "\n",
    "    return df_result, c\n",
    "\n",
    "\n",
    "def ccf_dedup_df(df):\n",
    "    df = df.dropDuplicates()\n",
    "    return df\n",
    "\n",
    "def ccf_iterate_sorting_df(df):\n",
    "\n",
    "    df = df.union(df.select([\"values\", \"key\"]))\n",
    "    df = df.groupBy(\"key\").agg(array_sort(collect_list(\"values\")).alias(\"value_list\"))\n",
    "    df = df.withColumn(\"min_val\", col(\"value_list\")[0])\n",
    "    \n",
    "    df = df.filter(col(\"min_val\") < col(\"key\"))\n",
    "\n",
    "    #emit1\n",
    "    df_min = df.select(col(\"key\"), col(\"min_val\").alias(\"values\"))\n",
    "    \n",
    "    #emit 2\n",
    "    df_new_pairs = df.withColumn(\"values\", explode(col(\"value_list\")))\n",
    "    df_new_pairs = df_new_pairs.filter(col(\"min_val\") < col(\"values\")).select(col(\"values\").alias(\"key\"), col(\"min_val\").alias(\"values\"))\n",
    "    \n",
    "    c = df_new_pairs.count()\n",
    "    # Union the two DataFrames\n",
    "    df_result = df_min.union(df_new_pairs)\n",
    "\n",
    "    return df_result, c\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd455266-d4c9-4e1b-ae33-2d5f4bdc3c3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cc84a4e-6a0c-46d5-a8f9-94503217a1f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main_loop_rdd(rdd, max_iteration, sorting=False):\n",
    "    c = 1\n",
    "    k = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    while c > 0:\n",
    "        if not sorting:\n",
    "            rdd, c = ccf_iterate_rdd(rdd)\n",
    "        else: \n",
    "            rdd, c = ccf_iterate_sorting_rdd(rdd)\n",
    "        rdd = ccf_dedup_rdd(rdd)\n",
    "        #print(\"iter\", k)\n",
    "        k = k + 1\n",
    "        #print('Iteration', k)\n",
    "        if k > max_iteration:\n",
    "\n",
    "            print(\"EXCEEDED MAX NUMBER OF ITERATION\")\n",
    "            break \n",
    "    #print(\"ended while\")\n",
    "    time_ccf = time.time() - start_time\n",
    "\n",
    "    return rdd, time_ccf, k\n",
    "\n",
    "def main_loop_df(df, max_iteration, sorting=False):\n",
    "    c = 1\n",
    "    k = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    while c > 0:\n",
    "        if not sorting:\n",
    "            df, c = ccf_iterate_df(df)\n",
    "        else: \n",
    "            df, c = ccf_iterate_sorting_df(df)\n",
    "        df = ccf_dedup_df(df)\n",
    "        #print(\"iter\", k)\n",
    "        k = k + 1\n",
    "        #print('Iteration df', k)\n",
    "        if k > max_iteration:\n",
    "            print(\"EXCEEDED MAX NUMBER OF ITERATION\")\n",
    "            break \n",
    "    #print(\"Ended while\")\n",
    "    time_ccf = time.time() - start_time\n",
    "\n",
    "    return df, time_ccf, k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14607e71-e2f8-4ebb-bbd7-5c86decded02",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Sanity Check\n",
    "\n",
    "We run a test on the graph presented in the white paper. It should take 4 iterations to parse it. \n",
    "And the out put should be: (2,1), (3,1), (4,1), (5,1), (7,6), (8,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "642b2ad7-fde8-414b-9669-ae9515ccf3ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#sanity check\n",
    "rdd_test = sc.parallelize([(1, 2), (2, 3), (2, 4), (4, 5), (6, 7), (7, 8)])\n",
    "df_test = rdd_test.toDF([\"key\", \"values\"])\n",
    "\n",
    "df_out, time_, k = main_loop_df(df_test, 10, True)\n",
    "\n",
    "print(\"Iterations: \", k)\n",
    "#df_out.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0756fa74-f8c7-4898-8a54-57a6be57d4f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd_out, time_, k = main_loop_rdd(rdd_test, 10, False)\n",
    "\n",
    "print(\"Iterations: \", k)\n",
    "rdd_out.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb9bb7a-5af6-4deb-8bf3-2c2d5f49f222",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Experimental Analysis\n",
    "Experimental analysis comparing the RDD and DataFrame versions\n",
    "is conducted on graphs of increasing size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc5606fc-0b44-4ad0-8962-74ffd9715e9a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Run modulations of our program on graphs of increasing size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c1f032f-ec86-4c6e-ba94-5d5bab4551f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = {'DataType': [], 'Sorting': [], 'Size': [],'Iterations': [], 'Time(s)': []}\n",
    "\n",
    "max_iterate = 10000\n",
    "data_size = 1000\n",
    "\n",
    "\n",
    "#increase the size of the rrd to measure impact in time \n",
    "for n in np.arange(1, data_size, step=100):\n",
    "\n",
    "    df_sample = df.limit(int(n))\n",
    "    rdd_sample = df_sample.rdd\n",
    "\n",
    "    #rdd without sorting\n",
    "    #print(\"start iter rdd no sort\")\n",
    "    rdd_out, time_ccf, k = main_loop_rdd(rdd_sample, max_iterate, False)\n",
    "\n",
    "    results['DataType'].append('RDD')\n",
    "    results['Sorting'].append('False')\n",
    "    results['Iterations'].append(k)\n",
    "    results['Time(s)'].append(time_ccf)\n",
    "    results[\"Size\"].append(n)\n",
    "\n",
    "    #rdd with sorting\n",
    "    #print(\"start iter rdd sort\")\n",
    "    rdd_out, time_ccf, k = main_loop_rdd(rdd_sample, max_iterate, True)\n",
    "\n",
    "    results['DataType'].append('RDD')\n",
    "    results['Sorting'].append('True')\n",
    "    results['Iterations'].append(k)\n",
    "    results['Time(s)'].append(time_ccf)\n",
    "    results[\"Size\"].append(n)\n",
    "\n",
    "    #DF without sorting\n",
    "    #print(\"start iter df\")\n",
    "    df_out, time_ccf, k = main_loop_df(df_sample, max_iterate, False)\n",
    "\n",
    "    results['DataType'].append('DataFrame')\n",
    "    results['Sorting'].append('False')\n",
    "    results['Iterations'].append(k)\n",
    "    results['Time(s)'].append(time_ccf)\n",
    "    results[\"Size\"].append(n)\n",
    "\n",
    "    #DF with sorting\n",
    "    #print(\"start iter df sort\")\n",
    "    df_out, time_ccf, k = main_loop_df(df_sample, max_iterate, True)\n",
    "\n",
    "    results['DataType'].append('DataFrame')\n",
    "    results['Sorting'].append('True')\n",
    "    results['Iterations'].append(k)\n",
    "    results['Time(s)'].append(time_ccf)\n",
    "    results[\"Size\"].append(n)\n",
    "\n",
    "    print(\"Sample size\", n, \"parsed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55ad6544-cbb4-4c0f-8887-fa6d95ce6098",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Display results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4178f86b-f2d3-4db5-8dfb-e04ae14a93b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bc613c5-08c5-4dfc-97c8-b02577a30783",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#visual analysis \n",
    "\n",
    "# Define a dictionary to map unique values of 'DataType' to colors\n",
    "data_type_colors = {'RDD': 'blue', 'DataFrame': 'green'}\n",
    "\n",
    "# Define a dictionary to map unique values of 'Sort' to marker shapes\n",
    "sort_markers = {'True': 'o', 'False': 'x'}\n",
    "\n",
    "# Create a figure and axis for the plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Iterate through unique combinations of 'DataType' and 'Sort'\n",
    "for data_type, group1 in results_df.groupby('DataType'):\n",
    "    for sort, group2 in group1.groupby('Sorting'):\n",
    "        marker = sort_markers.get(sort, 'o')\n",
    "        color = data_type_colors.get(data_type, 'gray')\n",
    "        label = f'{data_type}, {sort}'\n",
    "        ax.scatter(group2['Size'], group2['Time(s)'], c=color, marker=marker, label=label)\n",
    "\n",
    "# Set labels and legend\n",
    "ax.set_xlabel('Size')\n",
    "ax.set_ylabel('Time')\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "David Benayoun_Bigdata_CCF",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
